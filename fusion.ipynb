{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOzavBJUHE5u",
        "outputId": "b8fd5fd0-703f-4592-e23d-751c031b1df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    print(\"Not running in Colab, skipping Google Drive mount.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ======================\n",
        "# CONFIG\n",
        "# ======================\n",
        "BASE      = Path(\"/content/drive/MyDrive/FakeAVCeleb\")\n",
        "DATA_DIR  = BASE/\"frames\"                                # frames/{sample_id}/frame_*.jpg\n",
        "TRAIN_CSV = BASE/\"train_fixed_fullschema_hashsplit.csv\"\n",
        "TEST_CSV  = BASE/\"test_fixed_fullschema_hashsplit.csv\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/FakeAVCeleb/mesonet.pth\"\n",
        "OUT_DIR   = BASE/\"fusion_video_meso\"                     # embeddings\n",
        "OUT_CSV   = BASE/\"fusion_video_meso_index.csv\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ======================\n",
        "# MODEL\n",
        "# ======================\n",
        "class Meso4(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, padding=1), nn.BatchNorm2d(8), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(8, 8, kernel_size=3, padding=1), nn.BatchNorm2d(8), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(16 * (IMG_SIZE // 16) * (IMG_SIZE // 16), 16)\n",
        "        self.fc2 = nn.Linear(16, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        feat = torch.relu(self.fc1(x))  # [B,16]\n",
        "        if return_feat:\n",
        "            return feat\n",
        "        feat = self.dropout(feat)\n",
        "        return self.fc2(feat)\n",
        "\n",
        "# reload model\n",
        "meso = Meso4(num_classes=2).to(DEVICE)\n",
        "meso.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "meso.eval()\n",
        "\n",
        "# ======================\n",
        "# TRANSFORM\n",
        "# ======================\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# ======================\n",
        "# EXPORT LOOP\n",
        "# ======================\n",
        "def export_split(df: pd.DataFrame, split: str):\n",
        "    rows = []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Export {split} emb\"):\n",
        "        sid = str(row['sample_id'])\n",
        "        label = int(row['video_fake'])\n",
        "        folder = DATA_DIR/sid\n",
        "        frames = sorted(folder.glob(\"frame_*.jpg\"))\n",
        "        if not frames:\n",
        "            continue\n",
        "\n",
        "        feats = []\n",
        "        with torch.no_grad():\n",
        "            for f in frames:\n",
        "                img = Image.open(f).convert('RGB')\n",
        "                img = val_test_transform(img).unsqueeze(0).to(DEVICE)  # [1,3,H,W]\n",
        "                feat = meso(img, return_feat=True).cpu().numpy()[0]\n",
        "                feats.append(feat)\n",
        "\n",
        "        emb = np.mean(feats, axis=0).astype(np.float32)  # [16]\n",
        "        path = OUT_DIR/f\"{sid}_meso16.npy\"\n",
        "        np.save(path, emb)\n",
        "        rows.append({\"sample_id\": sid,\n",
        "                     \"meso_emb_path\": str(path),\n",
        "                     \"label\": label,\n",
        "                     \"split\": split})\n",
        "    return rows\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)[[\"sample_id\",\"video_fake\"]]\n",
        "test_df  = pd.read_csv(TEST_CSV)[[\"sample_id\",\"video_fake\"]]\n",
        "\n",
        "all_rows = []\n",
        "all_rows += export_split(train_df, \"train\")\n",
        "all_rows += export_split(test_df, \"test\")\n",
        "\n",
        "pd.DataFrame(all_rows).to_csv(OUT_CSV, index=False)\n",
        "print(f\"✓ Saved {len(all_rows)} Meso4 video embeddings → {OUT_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nKZEj6iHUAq",
        "outputId": "280fd486-055e-4e67-86a2-90ac2f0d7866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Export train emb: 100%|██████████| 1593/1593 [1:42:18<00:00,  3.85s/it]\n",
            "Export test emb: 100%|██████████| 407/407 [24:39<00:00,  3.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved 2000 Meso4 video embeddings → /content/drive/MyDrive/FakeAVCeleb/fusion_video_meso_index.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import vgg19, VGG19_Weights\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "# ======================\n",
        "# CONFIG\n",
        "# ======================\n",
        "BASE_DIR   = Path(\"/content/drive/MyDrive/FakeAVCeleb\")\n",
        "TRAIN_CSV  = BASE_DIR/\"train_fixed_fullschema_hashsplit.csv\"\n",
        "TEST_CSV   = BASE_DIR/\"test_fixed_fullschema_hashsplit.csv\"\n",
        "AUDIO_ROOT = BASE_DIR/\"audio_wav\"\n",
        "\n",
        "BEST_PATH  = BASE_DIR/\"audio_vgg_3ch_best_robust.pth\"\n",
        "OUT_DIR    = BASE_DIR/\"fusion_audio_vgg\"\n",
        "OUT_CSV    = BASE_DIR/\"fusion_audio_vgg_index.csv\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SAMPLE_RATE= 16000\n",
        "N_MELS     = 128\n",
        "N_MFCC     = 20\n",
        "DURA_SEC   = 4.0\n",
        "TARGET_LEN = int(SAMPLE_RATE * DURA_SEC)\n",
        "\n",
        "# ======================\n",
        "# FEATURE EXTRACTION\n",
        "# ======================\n",
        "def _pad_or_trim(y, target_len):\n",
        "    if len(y) > target_len: return y[:target_len]\n",
        "    if len(y) < target_len: return np.pad(y, (0, target_len - len(y)))\n",
        "    return y\n",
        "\n",
        "def extract_audio_features(path: Path) -> torch.Tensor:\n",
        "    y, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
        "    y = _pad_or_trim(y, TARGET_LEN)\n",
        "\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS)\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-6)\n",
        "\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
        "    mfcc_norm = (mfcc - mfcc.min()) / (mfcc.max() - mfcc.min() + 1e-6)\n",
        "\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    mfcc_delta_norm = (mfcc_delta - mfcc_delta.min()) / (mfcc_delta.max() - mfcc_delta.min() + 1e-6)\n",
        "\n",
        "    mel_t   = torch.tensor(mel_norm, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "    mfcc_t  = torch.tensor(mfcc_norm, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "    delta_t = torch.tensor(mfcc_delta_norm, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    mfcc_r  = F.interpolate(mfcc_t,  size=(128, mel_t.shape[-1]), mode=\"bilinear\", align_corners=False)\n",
        "    delta_r = F.interpolate(delta_t, size=(128, mel_t.shape[-1]), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    feat = torch.cat([mel_t.squeeze(0), mfcc_r.squeeze(0), delta_r.squeeze(0)], dim=0)\n",
        "    feat = F.interpolate(feat.unsqueeze(0), size=(128,128), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
        "    return feat  # [3,128,128]\n",
        "\n",
        "# ======================\n",
        "# DATASET\n",
        "# ======================\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, df, audio_root: Path):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.audio_root = audio_root\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        sid = str(row[\"sample_id\"])\n",
        "        label = int(row[\"audio_fake\"])\n",
        "        feat = extract_audio_features(self.audio_root/f\"{sid}.wav\")\n",
        "        return feat, label, sid\n",
        "\n",
        "# ======================\n",
        "# MODEL (with feature hook)\n",
        "# ======================\n",
        "class AudioVGGFeatures(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.vgg = vgg19(weights=VGG19_Weights.DEFAULT)\n",
        "        self.vgg.features[0] = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.vgg.classifier[-1] = nn.Linear(4096, 1)\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        y = self.vgg.features(x)\n",
        "        y = self.vgg.avgpool(y)\n",
        "        y = torch.flatten(y, 1)\n",
        "        feat = self.vgg.classifier[:5](y)     # [B,4096]\n",
        "        if return_feat:\n",
        "            return feat\n",
        "        logit = self.vgg.classifier[5:](feat).squeeze(1)\n",
        "        return logit\n",
        "\n",
        "# ======================\n",
        "# LOAD MODEL\n",
        "# ======================\n",
        "model = AudioVGGFeatures().to(DEVICE)\n",
        "model.load_state_dict(torch.load(BEST_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(f\"Loaded weights ← {BEST_PATH}\")\n",
        "\n",
        "# ======================\n",
        "# EXPORT LOOP\n",
        "# ======================\n",
        "def export_split(df: pd.DataFrame, split: str):\n",
        "    ds = AudioDataset(df, AUDIO_ROOT)\n",
        "    dl = DataLoader(ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "    rows = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for xb, yb, sids in tqdm(dl, desc=f\"Export {split} emb\"):\n",
        "            xb = xb.to(DEVICE)\n",
        "            feats = model(xb, return_feat=True).cpu().numpy()  # [B,4096]\n",
        "            for sid, emb, lbl in zip(sids, feats, yb.numpy().astype(int)):\n",
        "                path = OUT_DIR/f\"{sid}_vgg4096.npy\"\n",
        "                np.save(path, emb.astype(np.float32))\n",
        "                rows.append({\n",
        "                    \"sample_id\": sid,\n",
        "                    \"audio_vgg_emb_path\": str(path),\n",
        "                    \"label\": lbl,\n",
        "                    \"split\": split\n",
        "                })\n",
        "    return rows\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)[[\"sample_id\",\"audio_fake\"]]\n",
        "test_df  = pd.read_csv(TEST_CSV)[[\"sample_id\",\"audio_fake\"]]\n",
        "\n",
        "all_rows = []\n",
        "all_rows += export_split(train_df, \"train\")\n",
        "all_rows += export_split(test_df, \"test\")\n",
        "\n",
        "pd.DataFrame(all_rows).to_csv(OUT_CSV, index=False)\n",
        "print(f\"✓ Saved {len(all_rows)} Audio VGG19 embeddings → {OUT_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMcB3zpME4tD",
        "outputId": "ff978c41-2fdc-4752-d5d9-f7415fc5ff22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 548M/548M [00:07<00:00, 74.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded weights ← /content/drive/MyDrive/FakeAVCeleb/audio_vgg_3ch_best_robust.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Export train emb: 100%|██████████| 200/200 [11:35<00:00,  3.48s/it]\n",
            "Export test emb: 100%|██████████| 51/51 [02:50<00:00,  3.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved 2000 Audio VGG19 embeddings → /content/drive/MyDrive/FakeAVCeleb/fusion_audio_vgg_index.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "DEVICE        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BASE_DIR      = Path('/content/drive/MyDrive/FakeAVCeleb')\n",
        "TRAIN_CSV     = BASE_DIR/'train_fixed_fullschema_hashsplit.csv'\n",
        "TEST_CSV      = BASE_DIR/'test_fixed_fullschema_hashsplit.csv'\n",
        "AUDIO_ROOT    = BASE_DIR/'audio_wav'\n",
        "CACHE_DIR     = BASE_DIR/'cache/audio_cnn_mel'\n",
        "BEST_PATH     = BASE_DIR/'audio_cnn_best.pth'\n",
        "\n",
        "OUT_DIR       = BASE_DIR/'fusion_audio_cnn'\n",
        "OUT_CSV       = BASE_DIR/'fusion_audio_cnn_index.csv'\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SAMPLE_RATE   = 16000\n",
        "N_MELS        = 128\n",
        "DURA_SEC      = 4.0\n",
        "TARGET_LEN    = int(SAMPLE_RATE * DURA_SEC)\n",
        "\n",
        "# --- Mel-Spectrogram extraction ---\n",
        "def audio_to_mel(path: Path) -> torch.Tensor:\n",
        "    y, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
        "    if len(y) > TARGET_LEN:\n",
        "        y = y[:TARGET_LEN]\n",
        "    else:\n",
        "        y = np.pad(y, (0, TARGET_LEN - len(y)))\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS)\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min() + 1e-6)\n",
        "    return torch.tensor(mel_norm, dtype=torch.float32)  # [128, T]\n",
        "\n",
        "# --- Dataset ---\n",
        "class MelDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, split: str):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.split = split\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        sid = str(row['sample_id'])\n",
        "        label = int(row['audio_fake'])\n",
        "        pt_path = CACHE_DIR/f\"{sid}.pt\"\n",
        "\n",
        "        if pt_path.exists():\n",
        "            mel = torch.load(pt_path)\n",
        "        else:\n",
        "            mel = audio_to_mel(AUDIO_ROOT/f\"{sid}.wav\")\n",
        "            torch.save(mel, pt_path)\n",
        "\n",
        "        img = mel.unsqueeze(0)   # [1,128,T]\n",
        "        return img, label, sid\n",
        "\n",
        "# --- Model (feature extractor) ---\n",
        "class AudioCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((4,4)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64*4*4, 128), nn.ReLU(), nn.Dropout(0.5)\n",
        "        )\n",
        "        self.head = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        feat = self.cnn(x)          # [B,128]\n",
        "        if return_feat:\n",
        "            return feat\n",
        "        return self.head(feat).squeeze(1)\n",
        "\n",
        "# --- Load model ---\n",
        "model = AudioCNN().to(DEVICE)\n",
        "model.load_state_dict(torch.load(BEST_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "print(f\"Loaded best weights ← {BEST_PATH.name}\")\n",
        "\n",
        "# --- Export embeddings ---\n",
        "def export_split(df: pd.DataFrame, split: str):\n",
        "    ds = MelDataset(df, split)\n",
        "    dl = DataLoader(ds, batch_size=8, shuffle=False, num_workers=2)\n",
        "    rows = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb, sids in tqdm(dl, desc=f\"Export {split} emb\"):\n",
        "            xb = xb.to(DEVICE)\n",
        "            feats = model(xb, return_feat=True).cpu().numpy()  # [B,128]\n",
        "            for sid, emb, lbl in zip(sids, feats, yb.numpy().astype(int)):\n",
        "                path = OUT_DIR/f\"{sid}_cnn128.npy\"\n",
        "                np.save(path, emb.astype(np.float32))\n",
        "                rows.append({\n",
        "                    \"sample_id\": sid,\n",
        "                    \"audio_cnn_emb_path\": str(path),\n",
        "                    \"label\": lbl,\n",
        "                    \"split\": split\n",
        "                })\n",
        "    return rows\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)[['sample_id','audio_fake']]\n",
        "test_df  = pd.read_csv(TEST_CSV)[['sample_id','audio_fake']]\n",
        "\n",
        "all_rows = []\n",
        "all_rows += export_split(train_df, \"train\")\n",
        "all_rows += export_split(test_df, \"test\")\n",
        "\n",
        "pd.DataFrame(all_rows).to_csv(OUT_CSV, index=False)\n",
        "print(f\"✓ Saved {len(all_rows)} Audio CNN embeddings → {OUT_CSV}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urqkeoioNlUD",
        "outputId": "1ab338cd-9f81-43c5-b5ec-98964d3ab3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best weights ← audio_cnn_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Export train emb: 100%|██████████| 200/200 [03:48<00:00,  1.14s/it]\n",
            "Export test emb: 100%|██████████| 51/51 [01:01<00:00,  1.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Saved 2000 Audio CNN embeddings → /content/drive/MyDrive/FakeAVCeleb/fusion_audio_cnn_index.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load all indexes\n",
        "resnet_df = pd.read_csv(\"/content/drive/MyDrive/FakeAVCeleb/fusion_video_emb_index.csv\")\n",
        "meso_df   = pd.read_csv(\"/content/drive/MyDrive/FakeAVCeleb/fusion_video_meso_index.csv\")\n",
        "vgg_df    = pd.read_csv(\"/content/drive/MyDrive/FakeAVCeleb/fusion_audio_vgg_index.csv\")\n",
        "cnn_df    = pd.read_csv(\"/content/drive/MyDrive/FakeAVCeleb/fusion_audio_cnn_index.csv\")\n",
        "\n",
        "# Merge step by step\n",
        "df = resnet_df.merge(meso_df[[\"sample_id\",\"meso_emb_path\"]], on=\"sample_id\")\n",
        "df = df.merge(vgg_df[[\"sample_id\",\"audio_vgg_emb_path\"]], on=\"sample_id\")\n",
        "df = df.merge(cnn_df[[\"sample_id\",\"audio_cnn_emb_path\"]], on=\"sample_id\")\n",
        "\n",
        "# keep label/split from resnet_df (all aligned by hash split)\n",
        "print(df.head())\n",
        "print(\"Final merged shape:\", df.shape)\n",
        "\n",
        "df.to_csv(\"/content/drive/MyDrive/FakeAVCeleb/fusion_master_index.csv\", index=False)\n",
        "print(\"✓ Saved merged index → fusion_master_index.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H-VKvkCS4P6",
        "outputId": "60658c50-79e9-4fbe-bea3-f12929b701f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      sample_id                                     video_emb_path  label  \\\n",
            "0  sample_00991  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...      0   \n",
            "1  sample_00985  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...      0   \n",
            "2  sample_01173  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...      1   \n",
            "3  sample_01044  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...      1   \n",
            "4  sample_00838  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...      0   \n",
            "\n",
            "   split                                      meso_emb_path  \\\n",
            "0  train  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...   \n",
            "1  train  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...   \n",
            "2  train  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...   \n",
            "3  train  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...   \n",
            "4  train  /content/drive/MyDrive/FakeAVCeleb/fusion_vide...   \n",
            "\n",
            "                                  audio_vgg_emb_path  \\\n",
            "0  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...   \n",
            "1  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...   \n",
            "2  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...   \n",
            "3  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...   \n",
            "4  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...   \n",
            "\n",
            "                                  audio_cnn_emb_path  \n",
            "0  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...  \n",
            "1  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...  \n",
            "2  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...  \n",
            "3  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...  \n",
            "4  /content/drive/MyDrive/FakeAVCeleb/fusion_audi...  \n",
            "Final merged shape: (2000, 7)\n",
            "✓ Saved merged index → fusion_master_index.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INDEX_CSV = \"/content/drive/MyDrive/FakeAVCeleb/fusion_master_index.csv\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR = 1e-4\n",
        "WD = 1e-5\n",
        "\n",
        "# === Dataset ===\n",
        "class FusionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        # load each embedding\n",
        "        v_resnet = np.load(row[\"video_emb_path\"])\n",
        "        v_meso   = np.load(row[\"meso_emb_path\"])\n",
        "        a_vgg    = np.load(row[\"audio_vgg_emb_path\"])\n",
        "        a_cnn    = np.load(row[\"audio_cnn_emb_path\"])\n",
        "        # concat\n",
        "        feat = np.concatenate([v_resnet, v_meso, a_vgg, a_cnn]).astype(np.float32)\n",
        "        label = np.array(row[\"label\"], dtype=np.float32)\n",
        "        return torch.tensor(feat), torch.tensor(label)\n",
        "\n",
        "# === Model ===\n",
        "class FusionMLP(nn.Module):\n",
        "    def __init__(self, in_dim=4752):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x).squeeze(1)\n",
        "\n",
        "# === Data ===\n",
        "df = pd.read_csv(INDEX_CSV)\n",
        "train_df = df[df[\"split\"]==\"train\"]\n",
        "test_df  = df[df[\"split\"]==\"test\"]\n",
        "\n",
        "train_dl = DataLoader(FusionDataset(train_df), batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dl  = DataLoader(FusionDataset(test_df),  batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# === Train ===\n",
        "model = FusionMLP().to(DEVICE)\n",
        "crit = nn.BCEWithLogitsLoss()\n",
        "opt = optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "\n",
        "best_acc = 0\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train(); total_loss=0; preds=[]; labels=[]\n",
        "    for xb,yb in tqdm(train_dl, desc=f\"Epoch {epoch} Train\"):\n",
        "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = crit(out, yb)\n",
        "        loss.backward(); opt.step()\n",
        "        total_loss += loss.item()\n",
        "        preds += (torch.sigmoid(out)>0.5).detach().cpu().numpy().tolist()\n",
        "        labels += yb.cpu().numpy().tolist()\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    print(f\"→ Train Loss {total_loss/len(train_dl):.4f} Acc {acc:.3f}\")\n",
        "\n",
        "    # Eval\n",
        "    model.eval(); preds=[]; labels=[]\n",
        "    with torch.no_grad():\n",
        "        for xb,yb in test_dl:\n",
        "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "            out = model(xb)\n",
        "            preds += (torch.sigmoid(out)>0.5).cpu().numpy().tolist()\n",
        "            labels += yb.cpu().numpy().tolist()\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    print(f\"→ Test Acc {acc:.3f}\")\n",
        "    if acc>best_acc:\n",
        "        best_acc=acc\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/FakeAVCeleb/fusion_mlp_best.pth\")\n",
        "        print(\"✓ Saved new best fusion model\")\n",
        "\n",
        "# Final classification report\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/FakeAVCeleb/fusion_mlp_best.pth\"))\n",
        "model.eval(); preds=[]; labels=[]\n",
        "with torch.no_grad():\n",
        "    for xb,yb in test_dl:\n",
        "        xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        out = model(xb)\n",
        "        preds += (torch.sigmoid(out)>0.5).cpu().numpy().tolist()\n",
        "        labels += yb.cpu().numpy().tolist()\n",
        "print(classification_report(labels, preds, target_names=[\"Real\",\"Fake\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z0-OeP-VXcu",
        "outputId": "602cbc2a-e493-4ba3-8e37-a2180c4b9f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Train: 100%|██████████| 50/50 [22:46<00:00, 27.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.3814 Acc 0.892\n",
            "→ Test Acc 0.956\n",
            "✓ Saved new best fusion model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Train: 100%|██████████| 50/50 [00:20<00:00,  2.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.1456 Acc 0.952\n",
            "→ Test Acc 0.953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Train: 100%|██████████| 50/50 [00:17<00:00,  2.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.1239 Acc 0.955\n",
            "→ Test Acc 0.963\n",
            "✓ Saved new best fusion model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Train: 100%|██████████| 50/50 [00:21<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.1091 Acc 0.962\n",
            "→ Test Acc 0.963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Train: 100%|██████████| 50/50 [00:19<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.1084 Acc 0.966\n",
            "→ Test Acc 0.963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6 Train: 100%|██████████| 50/50 [00:19<00:00,  2.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.1059 Acc 0.965\n",
            "→ Test Acc 0.966\n",
            "✓ Saved new best fusion model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7 Train: 100%|██████████| 50/50 [00:18<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.0980 Acc 0.964\n",
            "→ Test Acc 0.963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8 Train: 100%|██████████| 50/50 [00:19<00:00,  2.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.0971 Acc 0.969\n",
            "→ Test Acc 0.966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9 Train: 100%|██████████| 50/50 [00:19<00:00,  2.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.0933 Acc 0.969\n",
            "→ Test Acc 0.968\n",
            "✓ Saved new best fusion model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10 Train: 100%|██████████| 50/50 [00:18<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ Train Loss 0.0955 Acc 0.968\n",
            "→ Test Acc 0.971\n",
            "✓ Saved new best fusion model\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.97      0.97      0.97       200\n",
            "        Fake       0.97      0.97      0.97       207\n",
            "\n",
            "    accuracy                           0.97       407\n",
            "   macro avg       0.97      0.97      0.97       407\n",
            "weighted avg       0.97      0.97      0.97       407\n",
            "\n"
          ]
        }
      ]
    }
  ]
}